{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/andrej.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply \"neurons.\" Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))\n",
    "print(features)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I generated data we can use to get the output of our simple network. This is all just random for now, going forward we'll start using normal data. Going through each relevant line:\n",
    "\n",
    "features = torch.randn((1, 5)) creates a tensor with shape (1, 5), one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one.\n",
    "\n",
    "weights = torch.randn_like(features) creates another tensor with the same shape as features, again containing values from a normal distribution.\n",
    "\n",
    "Finally, bias = torch.randn((1, 1)) creates a single value from a normal distribution.\n",
    "\n",
    "PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. In general, you'll use PyTorch tensors pretty much the same way you'd use Numpy arrays. They come with some nice benefits though such as GPU acceleration which we'll get to later. For now, use the generated data to calculate the output of this simple single layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Calculate the output of the network with input features features, weights weights, and bias bias. Similar to Numpy, PyTorch has a torch.sum() function, as well as a .sum() method on tensors, for taking sums. Use the function activation defined above as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "### Solution\n",
    "\n",
    "# Now, make our labels from our data and true weights\n",
    "\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "print(y)\n",
    "y = activation((features * weights).sum() + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better way is to use matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now just stack them up!\n",
    "\n",
    "That's how you can calculate the output for a single neuron. The real power of this algorithm happens when you start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Calculate the output for this multi-layer network using the weights W1 & W2, and the biases, B1 & B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up : NN using Numpy\n",
    "\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 1000\n",
      "1000 100\n",
      "64 100\n",
      "64 10\n",
      "0 26333579.574250415\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1): #change as per convenience\n",
    "  # Forward pass: compute predicted y\n",
    "  print(x.shape[0],x.shape[1])\n",
    "  print(w1.shape[0],w1.shape[1])\n",
    "  h = x.dot(w1)\n",
    "  #print(x.shape[0],x.shape[1])\n",
    "  print(h.shape[0],h.shape[1])\n",
    "  h_relu = np.maximum(h, 0)\n",
    "  #print(h_relu)\n",
    "  y_pred = h_relu.dot(w2)\n",
    "  print(y_pred.shape[0],y_pred.shape[1])\n",
    "  #print(w1.shape[0],w1.shape[1])\n",
    "  # Compute and print loss\n",
    "  loss = np.square(y_pred - y).sum()\n",
    "  print(t, loss)\n",
    "  \n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "  grad_h = grad_h_relu.copy()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.T.dot(grad_h)\n",
    " \n",
    "  # Update weights\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: NN using Tensors\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors; you should think of them as a generic tool for scientific computing.\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you use the device argument when constructing a Tensor to place the Tensor on a GPU.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we manually implement the forward and backward passes through the network, using operations on PyTorch Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31479762.0\n",
      "1 28913350.0\n",
      "2 29243552.0\n",
      "3 27895362.0\n",
      "4 23107436.0\n",
      "5 15990101.0\n",
      "6 9593524.0\n",
      "7 5352017.5\n",
      "8 3037138.75\n",
      "9 1860742.0\n",
      "10 1259786.875\n",
      "11 930937.125\n",
      "12 732243.25\n",
      "13 599410.5\n",
      "14 502805.65625\n",
      "15 428234.1875\n",
      "16 368409.4375\n",
      "17 319182.5\n",
      "18 278019.71875\n",
      "19 243234.453125\n",
      "20 213615.03125\n",
      "21 188273.046875\n",
      "22 166470.734375\n",
      "23 147623.84375\n",
      "24 131275.109375\n",
      "25 117030.7265625\n",
      "26 104576.8828125\n",
      "27 93658.5703125\n",
      "28 84056.0234375\n",
      "29 75581.828125\n",
      "30 68084.046875\n",
      "31 61440.05859375\n",
      "32 55534.92578125\n",
      "33 50277.61328125\n",
      "34 45588.796875\n",
      "35 41393.69921875\n",
      "36 37631.4765625\n",
      "37 34253.33203125\n",
      "38 31217.01171875\n",
      "39 28484.796875\n",
      "40 26017.4921875\n",
      "41 23789.0546875\n",
      "42 21770.765625\n",
      "43 19941.2109375\n",
      "44 18280.9375\n",
      "45 16772.931640625\n",
      "46 15401.443359375\n",
      "47 14152.92578125\n",
      "48 13015.2578125\n",
      "49 11977.388671875\n",
      "50 11029.626953125\n",
      "51 10163.5634765625\n",
      "52 9370.7041015625\n",
      "53 8644.6357421875\n",
      "54 7979.43701171875\n",
      "55 7370.23876953125\n",
      "56 6811.2939453125\n",
      "57 6299.21435546875\n",
      "58 5828.130859375\n",
      "59 5394.78857421875\n",
      "60 4996.0908203125\n",
      "61 4628.607421875\n",
      "62 4289.939453125\n",
      "63 3977.779296875\n",
      "64 3689.858154296875\n",
      "65 3423.87744140625\n",
      "66 3178.277099609375\n",
      "67 2951.233154296875\n",
      "68 2741.45361328125\n",
      "69 2547.442626953125\n",
      "70 2367.929443359375\n",
      "71 2201.71337890625\n",
      "72 2047.8311767578125\n",
      "73 1905.2933349609375\n",
      "74 1773.1234130859375\n",
      "75 1650.5789794921875\n",
      "76 1536.37646484375\n",
      "77 1430.46875\n",
      "78 1332.183349609375\n",
      "79 1241.00390625\n",
      "80 1156.33447265625\n",
      "81 1077.728515625\n",
      "82 1004.7100830078125\n",
      "83 936.8953247070312\n",
      "84 873.8228759765625\n",
      "85 815.2214965820312\n",
      "86 760.72119140625\n",
      "87 710.015869140625\n",
      "88 662.8580322265625\n",
      "89 618.9550170898438\n",
      "90 578.0723876953125\n",
      "91 539.9994506835938\n",
      "92 504.52978515625\n",
      "93 471.4815673828125\n",
      "94 440.6927185058594\n",
      "95 411.99139404296875\n",
      "96 385.2418212890625\n",
      "97 360.2786865234375\n",
      "98 337.0030517578125\n",
      "99 315.2796630859375\n",
      "100 295.0130615234375\n",
      "101 276.11077880859375\n",
      "102 258.4696044921875\n",
      "103 241.99945068359375\n",
      "104 226.61309814453125\n",
      "105 212.241943359375\n",
      "106 198.8123016357422\n",
      "107 186.26318359375\n",
      "108 174.53102111816406\n",
      "109 163.56814575195312\n",
      "110 153.31582641601562\n",
      "111 143.72691345214844\n",
      "112 134.75790405273438\n",
      "113 126.3690414428711\n",
      "114 118.52425384521484\n",
      "115 111.17960357666016\n",
      "116 104.30550384521484\n",
      "117 97.86922454833984\n",
      "118 91.84423065185547\n",
      "119 86.20231628417969\n",
      "120 80.91702270507812\n",
      "121 75.96913146972656\n",
      "122 71.33110046386719\n",
      "123 66.98513793945312\n",
      "124 62.91145706176758\n",
      "125 59.093448638916016\n",
      "126 55.5137825012207\n",
      "127 52.15864181518555\n",
      "128 49.011566162109375\n",
      "129 46.06288528442383\n",
      "130 43.29480743408203\n",
      "131 40.6983528137207\n",
      "132 38.26089859008789\n",
      "133 35.976253509521484\n",
      "134 33.830020904541016\n",
      "135 31.815227508544922\n",
      "136 29.92474365234375\n",
      "137 28.150480270385742\n",
      "138 26.483835220336914\n",
      "139 24.91876792907715\n",
      "140 23.448396682739258\n",
      "141 22.068256378173828\n",
      "142 20.770294189453125\n",
      "143 19.551481246948242\n",
      "144 18.4067440032959\n",
      "145 17.329486846923828\n",
      "146 16.318143844604492\n",
      "147 15.367103576660156\n",
      "148 14.472803115844727\n",
      "149 13.631881713867188\n",
      "150 12.84206485748291\n",
      "151 12.098554611206055\n",
      "152 11.399113655090332\n",
      "153 10.740605354309082\n",
      "154 10.121891021728516\n",
      "155 9.53941535949707\n",
      "156 8.99145221710205\n",
      "157 8.475728988647461\n",
      "158 7.9908528327941895\n",
      "159 7.533819675445557\n",
      "160 7.103461742401123\n",
      "161 6.698513031005859\n",
      "162 6.317122936248779\n",
      "163 5.957971572875977\n",
      "164 5.619812965393066\n",
      "165 5.301689624786377\n",
      "166 5.001842975616455\n",
      "167 4.7192463874816895\n",
      "168 4.4527974128723145\n",
      "169 4.202044486999512\n",
      "170 3.965272903442383\n",
      "171 3.7426514625549316\n",
      "172 3.532756805419922\n",
      "173 3.3344197273254395\n",
      "174 3.1482934951782227\n",
      "175 2.972012758255005\n",
      "176 2.806259870529175\n",
      "177 2.6498935222625732\n",
      "178 2.5023787021636963\n",
      "179 2.363192319869995\n",
      "180 2.231903314590454\n",
      "181 2.1081700325012207\n",
      "182 1.9914586544036865\n",
      "183 1.8811938762664795\n",
      "184 1.7772858142852783\n",
      "185 1.6793768405914307\n",
      "186 1.5868266820907593\n",
      "187 1.4992250204086304\n",
      "188 1.4169398546218872\n",
      "189 1.3391103744506836\n",
      "190 1.2657660245895386\n",
      "191 1.1963812112808228\n",
      "192 1.1308649778366089\n",
      "193 1.06902277469635\n",
      "194 1.0106451511383057\n",
      "195 0.9555705189704895\n",
      "196 0.9036055207252502\n",
      "197 0.8542375564575195\n",
      "198 0.8080123066902161\n",
      "199 0.7640350461006165\n",
      "200 0.7227140069007874\n",
      "201 0.68340665102005\n",
      "202 0.6464979648590088\n",
      "203 0.6116405725479126\n",
      "204 0.5785466432571411\n",
      "205 0.5472338199615479\n",
      "206 0.5178142786026001\n",
      "207 0.4899064898490906\n",
      "208 0.4634981155395508\n",
      "209 0.43857741355895996\n",
      "210 0.4150524437427521\n",
      "211 0.3928226828575134\n",
      "212 0.37178951501846313\n",
      "213 0.35187143087387085\n",
      "214 0.33309727907180786\n",
      "215 0.31531935930252075\n",
      "216 0.2985098659992218\n",
      "217 0.28256362676620483\n",
      "218 0.26753684878349304\n",
      "219 0.253286212682724\n",
      "220 0.23984694480895996\n",
      "221 0.22707156836986542\n",
      "222 0.21507155895233154\n",
      "223 0.2036009579896927\n",
      "224 0.19279822707176208\n",
      "225 0.18260402977466583\n",
      "226 0.17294932901859283\n",
      "227 0.163839191198349\n",
      "228 0.15516433119773865\n",
      "229 0.14698182046413422\n",
      "230 0.13922494649887085\n",
      "231 0.13190008699893951\n",
      "232 0.12495562434196472\n",
      "233 0.11838451772928238\n",
      "234 0.11217885464429855\n",
      "235 0.10630074143409729\n",
      "236 0.10072832554578781\n",
      "237 0.09545010328292847\n",
      "238 0.0904458686709404\n",
      "239 0.0857190266251564\n",
      "240 0.08123670518398285\n",
      "241 0.07699772715568542\n",
      "242 0.07297717034816742\n",
      "243 0.06918273866176605\n",
      "244 0.06559403240680695\n",
      "245 0.06217221915721893\n",
      "246 0.058928295969963074\n",
      "247 0.05588424205780029\n",
      "248 0.052985917776823044\n",
      "249 0.05023679882287979\n",
      "250 0.04761014133691788\n",
      "251 0.04516301676630974\n",
      "252 0.04280893877148628\n",
      "253 0.04059405252337456\n",
      "254 0.03851375728845596\n",
      "255 0.03650093451142311\n",
      "256 0.03462660312652588\n",
      "257 0.032857414335012436\n",
      "258 0.03116084635257721\n",
      "259 0.029567888006567955\n",
      "260 0.028046412393450737\n",
      "261 0.02659921534359455\n",
      "262 0.025238579139113426\n",
      "263 0.023950422182679176\n",
      "264 0.02272341400384903\n",
      "265 0.021562714129686356\n",
      "266 0.020456161350011826\n",
      "267 0.01941358670592308\n",
      "268 0.01842096447944641\n",
      "269 0.01749320700764656\n",
      "270 0.016605239361524582\n",
      "271 0.01575988344848156\n",
      "272 0.014964723028242588\n",
      "273 0.01422282587736845\n",
      "274 0.013491598889231682\n",
      "275 0.012819159775972366\n",
      "276 0.012174585834145546\n",
      "277 0.011561490595340729\n",
      "278 0.010987245477735996\n",
      "279 0.010432273149490356\n",
      "280 0.009911814704537392\n",
      "281 0.009425722062587738\n",
      "282 0.008956242352724075\n",
      "283 0.008509268052875996\n",
      "284 0.008088486269116402\n",
      "285 0.007691225036978722\n",
      "286 0.0073121413588523865\n",
      "287 0.006957565434277058\n",
      "288 0.006617104634642601\n",
      "289 0.006289197131991386\n",
      "290 0.005987375974655151\n",
      "291 0.005696926731616259\n",
      "292 0.005420528817921877\n",
      "293 0.005157913081347942\n",
      "294 0.004911839030683041\n",
      "295 0.004681059624999762\n",
      "296 0.004452298395335674\n",
      "297 0.004242328926920891\n",
      "298 0.0040424540638923645\n",
      "299 0.0038494891487061977\n",
      "300 0.0036697075702250004\n",
      "301 0.00349216815084219\n",
      "302 0.0033285771496593952\n",
      "303 0.0031742434948682785\n",
      "304 0.0030278784688562155\n",
      "305 0.0028861723840236664\n",
      "306 0.00275416043587029\n",
      "307 0.0026293755508959293\n",
      "308 0.002513718092814088\n",
      "309 0.002398132113739848\n",
      "310 0.0022906765807420015\n",
      "311 0.0021877302788197994\n",
      "312 0.0020924911368638277\n",
      "313 0.001999997068196535\n",
      "314 0.0019120237557217479\n",
      "315 0.0018285043770447373\n",
      "316 0.0017471866449341178\n",
      "317 0.001672935439273715\n",
      "318 0.0016014821594581008\n",
      "319 0.0015324712730944157\n",
      "320 0.0014696724247187376\n",
      "321 0.001405993476510048\n",
      "322 0.0013480267953127623\n",
      "323 0.0012918604770675302\n",
      "324 0.0012401221320033073\n",
      "325 0.0011897702934220433\n",
      "326 0.0011419549118727446\n",
      "327 0.0010948546696454287\n",
      "328 0.0010510911233723164\n",
      "329 0.0010095888283103704\n",
      "330 0.0009729510056786239\n",
      "331 0.0009327293373644352\n",
      "332 0.0008982945000752807\n",
      "333 0.0008630093652755022\n",
      "334 0.0008309785625897348\n",
      "335 0.0007994232000783086\n",
      "336 0.0007695745443925261\n",
      "337 0.0007401289767585695\n",
      "338 0.0007124026888050139\n",
      "339 0.0006869662320241332\n",
      "340 0.000662296311929822\n",
      "341 0.0006385609740391374\n",
      "342 0.0006161407800391316\n",
      "343 0.0005939261754974723\n",
      "344 0.0005735853919759393\n",
      "345 0.0005528434412553906\n",
      "346 0.0005346171674318612\n",
      "347 0.0005162462475709617\n",
      "348 0.000499196641612798\n",
      "349 0.0004811344260815531\n",
      "350 0.0004663243889808655\n",
      "351 0.0004503024974837899\n",
      "352 0.0004357858852017671\n",
      "353 0.000422078650444746\n",
      "354 0.0004084838437847793\n",
      "355 0.00039446671144105494\n",
      "356 0.0003824504092335701\n",
      "357 0.00036909652408212423\n",
      "358 0.00035836538881994784\n",
      "359 0.00034782884176820517\n",
      "360 0.00033630477264523506\n",
      "361 0.0003262143873143941\n",
      "362 0.00031641111127100885\n",
      "363 0.00030749241705052555\n",
      "364 0.0002974704257212579\n",
      "365 0.00028871733229607344\n",
      "366 0.0002799350186251104\n",
      "367 0.0002717047755140811\n",
      "368 0.0002634631819091737\n",
      "369 0.0002567930205259472\n",
      "370 0.0002492535568308085\n",
      "371 0.0002419194352114573\n",
      "372 0.0002357177872909233\n",
      "373 0.0002290735865244642\n",
      "374 0.00022260824334807694\n",
      "375 0.00021622490021400154\n",
      "376 0.00021061298321001232\n",
      "377 0.0002052966592600569\n",
      "378 0.00020020006923004985\n",
      "379 0.00019501461065374315\n",
      "380 0.00018986295617651194\n",
      "381 0.00018486383487470448\n",
      "382 0.00018036493565887213\n",
      "383 0.00017511448822915554\n",
      "384 0.00017033987387549132\n",
      "385 0.00016670090553816408\n",
      "386 0.000162016338435933\n",
      "387 0.00015821224951650947\n",
      "388 0.0001546704734209925\n",
      "389 0.00015104992780834436\n",
      "390 0.00014721593470312655\n",
      "391 0.00014325356460176408\n",
      "392 0.0001398776366841048\n",
      "393 0.00013631675392389297\n",
      "394 0.0001336924615316093\n",
      "395 0.00013044809747952968\n",
      "396 0.0001273498492082581\n",
      "397 0.0001240805722773075\n",
      "398 0.00012145435903221369\n",
      "399 0.00011911577166756615\n",
      "400 0.0001168059025076218\n",
      "401 0.0001137389917857945\n",
      "402 0.00011083210119977593\n",
      "403 0.00010869401739910245\n",
      "404 0.00010640551045071334\n",
      "405 0.00010425737855257466\n",
      "406 0.00010199897224083543\n",
      "407 0.00010020504123531282\n",
      "408 9.795200458029285e-05\n",
      "409 9.619278716854751e-05\n",
      "410 9.384707664139569e-05\n",
      "411 9.24050182220526e-05\n",
      "412 9.086703357752413e-05\n",
      "413 8.847150456858799e-05\n",
      "414 8.666700887260959e-05\n",
      "415 8.516159141436219e-05\n",
      "416 8.375506149604917e-05\n",
      "417 8.200841693906114e-05\n",
      "418 8.032951154746115e-05\n",
      "419 7.911171269370243e-05\n",
      "420 7.749701035209e-05\n",
      "421 7.611783075844869e-05\n",
      "422 7.4722760473378e-05\n",
      "423 7.334363181143999e-05\n",
      "424 7.210606418084353e-05\n",
      "425 7.085579272825271e-05\n",
      "426 6.964981730561703e-05\n",
      "427 6.816811219323426e-05\n",
      "428 6.701307574985549e-05\n",
      "429 6.58048375044018e-05\n",
      "430 6.446350744226947e-05\n",
      "431 6.358021346386522e-05\n",
      "432 6.231097358977422e-05\n",
      "433 6.114480493124574e-05\n",
      "434 6.0300859331618994e-05\n",
      "435 5.914361827308312e-05\n",
      "436 5.8098510635318235e-05\n",
      "437 5.715896622859873e-05\n",
      "438 5.6031360145425424e-05\n",
      "439 5.51621888007503e-05\n",
      "440 5.399261135607958e-05\n",
      "441 5.307427636580542e-05\n",
      "442 5.239477104623802e-05\n",
      "443 5.168151983525604e-05\n",
      "444 5.0714679673546925e-05\n",
      "445 5.004719059797935e-05\n",
      "446 4.9293405027128756e-05\n",
      "447 4.838192762690596e-05\n",
      "448 4.7709978389320895e-05\n",
      "449 4.683216320700012e-05\n",
      "450 4.608398012351245e-05\n",
      "451 4.549749428406358e-05\n",
      "452 4.492725929594599e-05\n",
      "453 4.4066953705623746e-05\n",
      "454 4.338270809967071e-05\n",
      "455 4.2654326534830034e-05\n",
      "456 4.2072129872394726e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 4.131835885345936e-05\n",
      "458 4.094160976819694e-05\n",
      "459 4.040946078021079e-05\n",
      "460 3.982945781899616e-05\n",
      "461 3.930013917852193e-05\n",
      "462 3.862919766106643e-05\n",
      "463 3.795621159952134e-05\n",
      "464 3.755572470254265e-05\n",
      "465 3.710310920723714e-05\n",
      "466 3.64943225577008e-05\n",
      "467 3.6176934372633696e-05\n",
      "468 3.579714393708855e-05\n",
      "469 3.5320430470164865e-05\n",
      "470 3.476472193142399e-05\n",
      "471 3.4370783396298066e-05\n",
      "472 3.378089240868576e-05\n",
      "473 3.342274794704281e-05\n",
      "474 3.2946394640021026e-05\n",
      "475 3.23609565384686e-05\n",
      "476 3.1946576200425625e-05\n",
      "477 3.1353974918602034e-05\n",
      "478 3.1048715754877776e-05\n",
      "479 3.069139347644523e-05\n",
      "480 3.019629730260931e-05\n",
      "481 2.9947921575512737e-05\n",
      "482 2.951656460936647e-05\n",
      "483 2.9135610020603053e-05\n",
      "484 2.910097646235954e-05\n",
      "485 2.8637467039516196e-05\n",
      "486 2.825950286933221e-05\n",
      "487 2.789109930745326e-05\n",
      "488 2.751221472863108e-05\n",
      "489 2.7171981855644844e-05\n",
      "490 2.6808334951056167e-05\n",
      "491 2.649083580763545e-05\n",
      "492 2.6120926122530364e-05\n",
      "493 2.586271148175001e-05\n",
      "494 2.552783553255722e-05\n",
      "495 2.5130539142992347e-05\n",
      "496 2.5036015358637087e-05\n",
      "497 2.4712815502425656e-05\n",
      "498 2.4380844479310326e-05\n",
      "499 2.416023926343769e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph and Autograd\n",
    "\n",
    "to be discussed in detail ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w 1  w.r.t to L: tensor(-36.)\n",
      "Gradient of w 2  w.r.t to L: tensor(-28.)\n",
      "Gradient of w 3  w.r.t to L: tensor(-8.)\n",
      "Gradient of w 4  w.r.t to L: tensor(-20.)\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w\",index,\" w.r.t to L:\", gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : NN using Autograd\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it's pretty simple to use in practice. If we want to compute gradients with respect to some Tensor, then we set requires_grad=True when constructing that Tensor. Any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation through the graph. If x is a Tensor with requires_grad=True, then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Sometimes you may wish to prevent PyTorch from building computational graphs when performing certain operations on Tensors with requires_grad=True; for example we usually don't want to backpropagate through the weight update steps when training a neural network. In such scenarios we can use the torch.no_grad() context manager to prevent the construction of a computational graph.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 25081558.0\n",
      "1 20200084.0\n",
      "2 18272832.0\n",
      "3 16990172.0\n",
      "4 15191287.0\n",
      "5 12619540.0\n",
      "6 9633979.0\n",
      "7 6852619.5\n",
      "8 4648788.5\n",
      "9 3100125.25\n",
      "10 2084139.0\n",
      "11 1442150.625\n",
      "12 1037781.3125\n",
      "13 779165.0\n",
      "14 608043.875\n",
      "15 490206.34375\n",
      "16 405412.15625\n",
      "17 341896.75\n",
      "18 292536.28125\n",
      "19 253008.453125\n",
      "20 220697.46875\n",
      "21 193750.890625\n",
      "22 171003.84375\n",
      "23 151558.171875\n",
      "24 134795.421875\n",
      "25 120268.53125\n",
      "26 107605.953125\n",
      "27 96524.984375\n",
      "28 86784.578125\n",
      "29 78198.078125\n",
      "30 70607.0625\n",
      "31 63885.109375\n",
      "32 57912.9765625\n",
      "33 52592.09375\n",
      "34 47845.59375\n",
      "35 43600.7578125\n",
      "36 39790.28515625\n",
      "37 36365.03515625\n",
      "38 33275.46875\n",
      "39 30492.4765625\n",
      "40 27976.66796875\n",
      "41 25699.41796875\n",
      "42 23635.6328125\n",
      "43 21763.5390625\n",
      "44 20061.4453125\n",
      "45 18511.5234375\n",
      "46 17098.708984375\n",
      "47 15809.1201171875\n",
      "48 14630.486328125\n",
      "49 13551.63671875\n",
      "50 12563.130859375\n",
      "51 11656.2734375\n",
      "52 10824.91796875\n",
      "53 10060.6552734375\n",
      "54 9357.2783203125\n",
      "55 8709.08203125\n",
      "56 8111.390625\n",
      "57 7559.80029296875\n",
      "58 7050.55078125\n",
      "59 6579.41796875\n",
      "60 6143.4189453125\n",
      "61 5739.65625\n",
      "62 5365.6357421875\n",
      "63 5018.9111328125\n",
      "64 4696.98193359375\n",
      "65 4397.796875\n",
      "66 4119.8056640625\n",
      "67 3861.221923828125\n",
      "68 3620.501220703125\n",
      "69 3396.33740234375\n",
      "70 3187.362060546875\n",
      "71 2992.3984375\n",
      "72 2810.456787109375\n",
      "73 2640.579345703125\n",
      "74 2481.917236328125\n",
      "75 2333.614501953125\n",
      "76 2194.891845703125\n",
      "77 2065.10693359375\n",
      "78 1943.5953369140625\n",
      "79 1829.833984375\n",
      "80 1723.25341796875\n",
      "81 1623.335693359375\n",
      "82 1529.62158203125\n",
      "83 1441.70068359375\n",
      "84 1359.205810546875\n",
      "85 1281.76708984375\n",
      "86 1209.0225830078125\n",
      "87 1140.705078125\n",
      "88 1076.4725341796875\n",
      "89 1016.0813598632812\n",
      "90 959.296875\n",
      "91 905.8756103515625\n",
      "92 855.61865234375\n",
      "93 808.321044921875\n",
      "94 763.7706298828125\n",
      "95 721.8143310546875\n",
      "96 682.294677734375\n",
      "97 645.0478515625\n",
      "98 609.95361328125\n",
      "99 576.8568115234375\n",
      "100 545.6425170898438\n",
      "101 516.2041625976562\n",
      "102 488.4278259277344\n",
      "103 462.20916748046875\n",
      "104 437.4745178222656\n",
      "105 414.1189270019531\n",
      "106 392.0617370605469\n",
      "107 371.2245788574219\n",
      "108 351.55126953125\n",
      "109 332.9594421386719\n",
      "110 315.3924255371094\n",
      "111 298.7850646972656\n",
      "112 283.0858154296875\n",
      "113 268.24273681640625\n",
      "114 254.20828247070312\n",
      "115 240.94212341308594\n",
      "116 228.38629150390625\n",
      "117 216.5074462890625\n",
      "118 205.2674102783203\n",
      "119 194.63211059570312\n",
      "120 184.57054138183594\n",
      "121 175.04190063476562\n",
      "122 166.01858520507812\n",
      "123 157.4746551513672\n",
      "124 149.38429260253906\n",
      "125 141.7251434326172\n",
      "126 134.47174072265625\n",
      "127 127.59656524658203\n",
      "128 121.08390045166016\n",
      "129 114.91197204589844\n",
      "130 109.06452941894531\n",
      "131 103.52471923828125\n",
      "132 98.27084350585938\n",
      "133 93.28962707519531\n",
      "134 88.56840515136719\n",
      "135 84.09292602539062\n",
      "136 79.84986877441406\n",
      "137 75.82560729980469\n",
      "138 72.00828552246094\n",
      "139 68.387939453125\n",
      "140 64.95304107666016\n",
      "141 61.69599151611328\n",
      "142 58.60578155517578\n",
      "143 55.67197799682617\n",
      "144 52.888702392578125\n",
      "145 50.248443603515625\n",
      "146 47.743282318115234\n",
      "147 45.36524200439453\n",
      "148 43.10661697387695\n",
      "149 40.96398162841797\n",
      "150 38.928585052490234\n",
      "151 36.99768829345703\n",
      "152 35.164459228515625\n",
      "153 33.4228515625\n",
      "154 31.76983070373535\n",
      "155 30.199167251586914\n",
      "156 28.708534240722656\n",
      "157 27.292579650878906\n",
      "158 25.947818756103516\n",
      "159 24.669509887695312\n",
      "160 23.45564079284668\n",
      "161 22.302494049072266\n",
      "162 21.206947326660156\n",
      "163 20.16655921936035\n",
      "164 19.177234649658203\n",
      "165 18.237844467163086\n",
      "166 17.344594955444336\n",
      "167 16.49631690979004\n",
      "168 15.689970970153809\n",
      "169 14.923310279846191\n",
      "170 14.194713592529297\n",
      "171 13.502538681030273\n",
      "172 12.84475326538086\n",
      "173 12.218988418579102\n",
      "174 11.624197006225586\n",
      "175 11.05880355834961\n",
      "176 10.52139663696289\n",
      "177 10.010343551635742\n",
      "178 9.524336814880371\n",
      "179 9.062570571899414\n",
      "180 8.623199462890625\n",
      "181 8.205288887023926\n",
      "182 7.808181285858154\n",
      "183 7.430413246154785\n",
      "184 7.07108211517334\n",
      "185 6.729176044464111\n",
      "186 6.404404640197754\n",
      "187 6.0954155921936035\n",
      "188 5.8007402420043945\n",
      "189 5.521284580230713\n",
      "190 5.255383491516113\n",
      "191 5.002501010894775\n",
      "192 4.761588096618652\n",
      "193 4.532399654388428\n",
      "194 4.314355850219727\n",
      "195 4.106954097747803\n",
      "196 3.9096193313598633\n",
      "197 3.7220280170440674\n",
      "198 3.543501377105713\n",
      "199 3.373552083969116\n",
      "200 3.211824893951416\n",
      "201 3.0579302310943604\n",
      "202 2.9114255905151367\n",
      "203 2.772038221359253\n",
      "204 2.6394548416137695\n",
      "205 2.513185739517212\n",
      "206 2.393209457397461\n",
      "207 2.2788071632385254\n",
      "208 2.1700387001037598\n",
      "209 2.0663070678710938\n",
      "210 1.9677757024765015\n",
      "211 1.8738805055618286\n",
      "212 1.7846102714538574\n",
      "213 1.699638843536377\n",
      "214 1.6186623573303223\n",
      "215 1.5416301488876343\n",
      "216 1.4682183265686035\n",
      "217 1.3983302116394043\n",
      "218 1.3318921327590942\n",
      "219 1.2685019969940186\n",
      "220 1.2081676721572876\n",
      "221 1.1508911848068237\n",
      "222 1.0963525772094727\n",
      "223 1.0442534685134888\n",
      "224 0.9947349429130554\n",
      "225 0.9475424289703369\n",
      "226 0.9026670455932617\n",
      "227 0.8598316311836243\n",
      "228 0.8190736174583435\n",
      "229 0.7802920341491699\n",
      "230 0.7433682084083557\n",
      "231 0.708251953125\n",
      "232 0.6747331619262695\n",
      "233 0.6428097486495972\n",
      "234 0.6124694347381592\n",
      "235 0.5834445953369141\n",
      "236 0.5559720993041992\n",
      "237 0.5296372175216675\n",
      "238 0.5046608448028564\n",
      "239 0.48086920380592346\n",
      "240 0.4581782817840576\n",
      "241 0.43656009435653687\n",
      "242 0.41596174240112305\n",
      "243 0.3964211642742157\n",
      "244 0.37773361802101135\n",
      "245 0.36000698804855347\n",
      "246 0.34300678968429565\n",
      "247 0.32684415578842163\n",
      "248 0.31148332357406616\n",
      "249 0.29683101177215576\n",
      "250 0.28288787603378296\n",
      "251 0.26952889561653137\n",
      "252 0.2569067180156708\n",
      "253 0.24481654167175293\n",
      "254 0.23333987593650818\n",
      "255 0.22239091992378235\n",
      "256 0.21191641688346863\n",
      "257 0.20203560590744019\n",
      "258 0.19254541397094727\n",
      "259 0.18354004621505737\n",
      "260 0.1749216765165329\n",
      "261 0.1667039394378662\n",
      "262 0.1589161902666092\n",
      "263 0.15144607424736023\n",
      "264 0.14437471330165863\n",
      "265 0.13762101531028748\n",
      "266 0.13116368651390076\n",
      "267 0.1250261813402176\n",
      "268 0.11920059472322464\n",
      "269 0.11361315101385117\n",
      "270 0.1083383858203888\n",
      "271 0.1032438725233078\n",
      "272 0.09844110906124115\n",
      "273 0.09385554492473602\n",
      "274 0.08946467936038971\n",
      "275 0.08531446009874344\n",
      "276 0.08132421970367432\n",
      "277 0.07754088938236237\n",
      "278 0.07389522343873978\n",
      "279 0.07045409828424454\n",
      "280 0.06718944013118744\n",
      "281 0.06405964493751526\n",
      "282 0.06106753647327423\n",
      "283 0.05823127180337906\n",
      "284 0.05554003268480301\n",
      "285 0.05294349044561386\n",
      "286 0.05047763139009476\n",
      "287 0.04815514013171196\n",
      "288 0.04591231793165207\n",
      "289 0.04378550872206688\n",
      "290 0.04173825681209564\n",
      "291 0.03980575501918793\n",
      "292 0.03796621784567833\n",
      "293 0.03619806095957756\n",
      "294 0.03452455252408981\n",
      "295 0.0329270176589489\n",
      "296 0.03139769285917282\n",
      "297 0.029956145212054253\n",
      "298 0.02857876569032669\n",
      "299 0.027243515476584435\n",
      "300 0.025996891781687737\n",
      "301 0.02478742226958275\n",
      "302 0.02365792542695999\n",
      "303 0.022565927356481552\n",
      "304 0.021532565355300903\n",
      "305 0.02054673805832863\n",
      "306 0.01960132271051407\n",
      "307 0.018704025074839592\n",
      "308 0.017840903252363205\n",
      "309 0.017037544399499893\n",
      "310 0.016249865293502808\n",
      "311 0.015496493317186832\n",
      "312 0.014780731871724129\n",
      "313 0.014115124940872192\n",
      "314 0.013476726599037647\n",
      "315 0.012861441820859909\n",
      "316 0.012279235757887363\n",
      "317 0.011723936535418034\n",
      "318 0.011198296211659908\n",
      "319 0.010689232498407364\n",
      "320 0.010202599689364433\n",
      "321 0.009761035442352295\n",
      "322 0.00931314006447792\n",
      "323 0.008892781101167202\n",
      "324 0.00849038828164339\n",
      "325 0.008116532117128372\n",
      "326 0.007755941245704889\n",
      "327 0.007410841528326273\n",
      "328 0.007080219220370054\n",
      "329 0.006770756561309099\n",
      "330 0.0064774006605148315\n",
      "331 0.006189253646880388\n",
      "332 0.005917244590818882\n",
      "333 0.005657284054905176\n",
      "334 0.00541085796430707\n",
      "335 0.005178043153136969\n",
      "336 0.004955293610692024\n",
      "337 0.004742850549519062\n",
      "338 0.004537926986813545\n",
      "339 0.0043430691584944725\n",
      "340 0.004153785295784473\n",
      "341 0.003976623527705669\n",
      "342 0.0038097593933343887\n",
      "343 0.0036475935485213995\n",
      "344 0.003495117649435997\n",
      "345 0.0033498178236186504\n",
      "346 0.0032096519134938717\n",
      "347 0.003074764972552657\n",
      "348 0.0029501758981496096\n",
      "349 0.002824355848133564\n",
      "350 0.002709396881982684\n",
      "351 0.002600414212793112\n",
      "352 0.002492898376658559\n",
      "353 0.0023920454550534487\n",
      "354 0.0022961366921663284\n",
      "355 0.0022044843062758446\n",
      "356 0.0021168766543269157\n",
      "357 0.0020322739146649837\n",
      "358 0.0019522177753970027\n",
      "359 0.0018744030967354774\n",
      "360 0.0018009921768680215\n",
      "361 0.0017296446021646261\n",
      "362 0.0016641879919916391\n",
      "363 0.0015991987893357873\n",
      "364 0.001539766089990735\n",
      "365 0.0014821888180449605\n",
      "366 0.0014244928024709225\n",
      "367 0.001370571437291801\n",
      "368 0.001319997594691813\n",
      "369 0.0012709852308034897\n",
      "370 0.0012236661277711391\n",
      "371 0.0011821282096207142\n",
      "372 0.001138614839874208\n",
      "373 0.0010983932297676802\n",
      "374 0.0010606828145682812\n",
      "375 0.001023623626679182\n",
      "376 0.000985607854090631\n",
      "377 0.0009519397863186896\n",
      "378 0.0009184301597997546\n",
      "379 0.0008867039578035474\n",
      "380 0.0008576139807701111\n",
      "381 0.0008277479792013764\n",
      "382 0.0007998357759788632\n",
      "383 0.000773377250880003\n",
      "384 0.0007486659451387823\n",
      "385 0.0007234356016851962\n",
      "386 0.0007000869372859597\n",
      "387 0.0006764887366443872\n",
      "388 0.0006568753742612898\n",
      "389 0.0006353993085213006\n",
      "390 0.0006145358202047646\n",
      "391 0.0005962756695225835\n",
      "392 0.0005782044027000666\n",
      "393 0.0005605265032500029\n",
      "394 0.0005425984272733331\n",
      "395 0.0005267768865451217\n",
      "396 0.0005108818295411766\n",
      "397 0.0004958045901730657\n",
      "398 0.00048094126395881176\n",
      "399 0.00046626583207398653\n",
      "400 0.000452159671112895\n",
      "401 0.0004389870446175337\n",
      "402 0.0004265016468707472\n",
      "403 0.0004149794694967568\n",
      "404 0.0004029551928397268\n",
      "405 0.00039134567487053573\n",
      "406 0.0003803894214797765\n",
      "407 0.0003697617503348738\n",
      "408 0.0003603651712182909\n",
      "409 0.0003507738874759525\n",
      "410 0.0003409715718589723\n",
      "411 0.00033160089515149593\n",
      "412 0.0003223305393476039\n",
      "413 0.0003135031438432634\n",
      "414 0.00030589717789553106\n",
      "415 0.0002977868134621531\n",
      "416 0.00029024522518739104\n",
      "417 0.0002822180977091193\n",
      "418 0.0002753670560196042\n",
      "419 0.0002687142987269908\n",
      "420 0.000262272369582206\n",
      "421 0.0002555744431447238\n",
      "422 0.0002492144703865051\n",
      "423 0.00024346541613340378\n",
      "424 0.00023733102716505527\n",
      "425 0.00023096310906112194\n",
      "426 0.0002253392740385607\n",
      "427 0.00022027289378456771\n",
      "428 0.00021455758542288095\n",
      "429 0.00020935467910021544\n",
      "430 0.00020458766084630042\n",
      "431 0.00019963824888691306\n",
      "432 0.0001952332240762189\n",
      "433 0.00019062223145738244\n",
      "434 0.0001860072952695191\n",
      "435 0.0001818905002437532\n",
      "436 0.0001781533646862954\n",
      "437 0.00017456253408454359\n",
      "438 0.00017032089817803353\n",
      "439 0.00016657161177136004\n",
      "440 0.00016316623077727854\n",
      "441 0.00015939242439344525\n",
      "442 0.00015612744027748704\n",
      "443 0.0001534939365228638\n",
      "444 0.0001504909887444228\n",
      "445 0.00014632994134444743\n",
      "446 0.00014385754184331745\n",
      "447 0.00014063631533645093\n",
      "448 0.0001376744476146996\n",
      "449 0.00013515538012143224\n",
      "450 0.00013217778177931905\n",
      "451 0.00012917241838295013\n",
      "452 0.00012678692291956395\n",
      "453 0.00012417846301104873\n",
      "454 0.00012190730922156945\n",
      "455 0.000119623655336909\n",
      "456 0.00011725555668817833\n",
      "457 0.0001150600437540561\n",
      "458 0.0001133292171289213\n",
      "459 0.0001111093006329611\n",
      "460 0.00010910688433796167\n",
      "461 0.00010681570711312816\n",
      "462 0.00010485705570317805\n",
      "463 0.00010280031710863113\n",
      "464 0.00010091288277180865\n",
      "465 9.926203347276896e-05\n",
      "466 9.745269198901951e-05\n",
      "467 9.589671390131116e-05\n",
      "468 9.41743201110512e-05\n",
      "469 9.221903019351885e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470 9.059591684490442e-05\n",
      "471 8.933657954912633e-05\n",
      "472 8.79260478541255e-05\n",
      "473 8.647232607472688e-05\n",
      "474 8.497940143570304e-05\n",
      "475 8.328822150360793e-05\n",
      "476 8.182979217963293e-05\n",
      "477 8.051382610574365e-05\n",
      "478 7.916875620139763e-05\n",
      "479 7.806986104696989e-05\n",
      "480 7.667797763133422e-05\n",
      "481 7.518579513998702e-05\n",
      "482 7.424650539178401e-05\n",
      "483 7.314662798307836e-05\n",
      "484 7.190664473455399e-05\n",
      "485 7.088413258315995e-05\n",
      "486 6.977178418310359e-05\n",
      "487 6.870286597404629e-05\n",
      "488 6.787486927350983e-05\n",
      "489 6.675174518022686e-05\n",
      "490 6.570119876414537e-05\n",
      "491 6.474748806795105e-05\n",
      "492 6.394041702151299e-05\n",
      "493 6.27890694886446e-05\n",
      "494 6.208664854057133e-05\n",
      "495 6.125254731159657e-05\n",
      "496 6.039383151801303e-05\n",
      "497 5.953175786999054e-05\n",
      "498 5.854872506461106e-05\n",
      "499 5.796084224130027e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "#device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't worry if you didn't understand anything above, everything will be explanied much clearly as we go through the next set of notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
