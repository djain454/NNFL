{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n",
    "\n",
    "In this notebook, you'll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/andrej.png\" width=700px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply \"neurons.\" Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit's output.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" width=400px>\n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "With the basics covered, it's time to explore how we can use PyTorch to build a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 5 random normal variables\n",
    "features = torch.randn((1, 5))\n",
    "# True weights for our data, random normal variables again\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1, 1))\n",
    "print(features)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I generated data we can use to get the output of our simple network. This is all just random for now, going forward we'll start using normal data. Going through each relevant line:\n",
    "\n",
    "features = torch.randn((1, 5)) creates a tensor with shape (1, 5), one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one.\n",
    "\n",
    "weights = torch.randn_like(features) creates another tensor with the same shape as features, again containing values from a normal distribution.\n",
    "\n",
    "Finally, bias = torch.randn((1, 1)) creates a single value from a normal distribution.\n",
    "\n",
    "PyTorch tensors can be added, multiplied, subtracted, etc, just like Numpy arrays. In general, you'll use PyTorch tensors pretty much the same way you'd use Numpy arrays. They come with some nice benefits though such as GPU acceleration which we'll get to later. For now, use the generated data to calculate the output of this simple single layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Calculate the output of the network with input features features, weights weights, and bias bias. Similar to Numpy, PyTorch has a torch.sum() function, as well as a .sum() method on tensors, for taking sums. Use the function activation defined above as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "### Solution\n",
    "\n",
    "# Now, make our labels from our data and true weights\n",
    "\n",
    "y = activation(torch.sum(features * weights) + bias)\n",
    "print(y)\n",
    "y = activation((features * weights).sum() + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much better way is to use matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Solution\n",
    "y = activation(torch.mm(features, weights.view(5,1)) + bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now just stack them up!\n",
    "\n",
    "That's how you can calculate the output for a single neuron. The real power of this algorithm happens when you start stacking these individual units into layers and stacks of layers, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' width=450px>\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "n_input = features.shape[1]     # Number of input units, must match number of input features\n",
    "n_hidden = 2                    # Number of hidden units \n",
    "n_output = 1                    # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Weights for hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Calculate the output for this multi-layer network using the weights W1 & W2, and the biases, B1 & B2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Up : NN using Numpy\n",
    "\n",
    "Before introducing PyTorch, we will first implement the network using numpy.\n",
    "\n",
    "Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a two-layer network to random data by manually implementing the forward and backward passes through the network using numpy operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 1000\n",
      "1000 100\n",
      "64 100\n",
      "64 10\n",
      "0 32906400.232589435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(1): #change as per convenience\n",
    "  # Forward pass: compute predicted y\n",
    "  print(x.shape[0],x.shape[1])\n",
    "  print(w1.shape[0],w1.shape[1])\n",
    "  h = x.dot(w1)\n",
    "  #print(x.shape[0],x.shape[1])\n",
    "  print(h.shape[0],h.shape[1])\n",
    "  h_relu = np.maximum(h, 0)\n",
    "  #print(h_relu)\n",
    "  y_pred = h_relu.dot(w2)\n",
    "  print(y_pred.shape[0],y_pred.shape[1])\n",
    "  #print(w1.shape[0],w1.shape[1])\n",
    "  # Compute and print loss\n",
    "  loss = np.square(y_pred - y).sum()\n",
    "  print(t, loss)\n",
    "  \n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "  grad_h = grad_h_relu.copy()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.T.dot(grad_h)\n",
    " \n",
    "  # Update weights\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: NN using Tensors\n",
    "\n",
    "Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won't be enough for modern deep learning.\n",
    "\n",
    "Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Any computation you might want to perform with numpy can also be accomplished with PyTorch Tensors; you should think of them as a generic tool for scientific computing.\n",
    "\n",
    "However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you use the device argument when constructing a Tensor to place the Tensor on a GPU.\n",
    "\n",
    "Here we use PyTorch Tensors to fit a two-layer network to random data. Like the numpy example above we manually implement the forward and backward passes through the network, using operations on PyTorch Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31479762.0\n",
      "1 28913350.0\n",
      "2 29243552.0\n",
      "3 27895362.0\n",
      "4 23107436.0\n",
      "5 15990101.0\n",
      "6 9593524.0\n",
      "7 5352017.5\n",
      "8 3037138.75\n",
      "9 1860742.0\n",
      "10 1259786.875\n",
      "11 930937.125\n",
      "12 732243.25\n",
      "13 599410.5\n",
      "14 502805.65625\n",
      "15 428234.1875\n",
      "16 368409.4375\n",
      "17 319182.5\n",
      "18 278019.71875\n",
      "19 243234.453125\n",
      "20 213615.03125\n",
      "21 188273.046875\n",
      "22 166470.734375\n",
      "23 147623.84375\n",
      "24 131275.109375\n",
      "25 117030.7265625\n",
      "26 104576.8828125\n",
      "27 93658.5703125\n",
      "28 84056.0234375\n",
      "29 75581.828125\n",
      "30 68084.046875\n",
      "31 61440.05859375\n",
      "32 55534.92578125\n",
      "33 50277.61328125\n",
      "34 45588.796875\n",
      "35 41393.69921875\n",
      "36 37631.4765625\n",
      "37 34253.33203125\n",
      "38 31217.01171875\n",
      "39 28484.796875\n",
      "40 26017.4921875\n",
      "41 23789.0546875\n",
      "42 21770.765625\n",
      "43 19941.2109375\n",
      "44 18280.9375\n",
      "45 16772.931640625\n",
      "46 15401.443359375\n",
      "47 14152.92578125\n",
      "48 13015.2578125\n",
      "49 11977.388671875\n",
      "50 11029.626953125\n",
      "51 10163.5634765625\n",
      "52 9370.7041015625\n",
      "53 8644.6357421875\n",
      "54 7979.43701171875\n",
      "55 7370.23876953125\n",
      "56 6811.2939453125\n",
      "57 6299.21435546875\n",
      "58 5828.130859375\n",
      "59 5394.78857421875\n",
      "60 4996.0908203125\n",
      "61 4628.607421875\n",
      "62 4289.939453125\n",
      "63 3977.779296875\n",
      "64 3689.858154296875\n",
      "65 3423.87744140625\n",
      "66 3178.277099609375\n",
      "67 2951.233154296875\n",
      "68 2741.45361328125\n",
      "69 2547.442626953125\n",
      "70 2367.929443359375\n",
      "71 2201.71337890625\n",
      "72 2047.8311767578125\n",
      "73 1905.2933349609375\n",
      "74 1773.1234130859375\n",
      "75 1650.5789794921875\n",
      "76 1536.37646484375\n",
      "77 1430.46875\n",
      "78 1332.183349609375\n",
      "79 1241.00390625\n",
      "80 1156.33447265625\n",
      "81 1077.728515625\n",
      "82 1004.7100830078125\n",
      "83 936.8953247070312\n",
      "84 873.8228759765625\n",
      "85 815.2214965820312\n",
      "86 760.72119140625\n",
      "87 710.015869140625\n",
      "88 662.8580322265625\n",
      "89 618.9550170898438\n",
      "90 578.0723876953125\n",
      "91 539.9994506835938\n",
      "92 504.52978515625\n",
      "93 471.4815673828125\n",
      "94 440.6927185058594\n",
      "95 411.99139404296875\n",
      "96 385.2418212890625\n",
      "97 360.2786865234375\n",
      "98 337.0030517578125\n",
      "99 315.2796630859375\n",
      "100 295.0130615234375\n",
      "101 276.11077880859375\n",
      "102 258.4696044921875\n",
      "103 241.99945068359375\n",
      "104 226.61309814453125\n",
      "105 212.241943359375\n",
      "106 198.8123016357422\n",
      "107 186.26318359375\n",
      "108 174.53102111816406\n",
      "109 163.56814575195312\n",
      "110 153.31582641601562\n",
      "111 143.72691345214844\n",
      "112 134.75790405273438\n",
      "113 126.3690414428711\n",
      "114 118.52425384521484\n",
      "115 111.17960357666016\n",
      "116 104.30550384521484\n",
      "117 97.86922454833984\n",
      "118 91.84423065185547\n",
      "119 86.20231628417969\n",
      "120 80.91702270507812\n",
      "121 75.96913146972656\n",
      "122 71.33110046386719\n",
      "123 66.98513793945312\n",
      "124 62.91145706176758\n",
      "125 59.093448638916016\n",
      "126 55.5137825012207\n",
      "127 52.15864181518555\n",
      "128 49.011566162109375\n",
      "129 46.06288528442383\n",
      "130 43.29480743408203\n",
      "131 40.6983528137207\n",
      "132 38.26089859008789\n",
      "133 35.976253509521484\n",
      "134 33.830020904541016\n",
      "135 31.815227508544922\n",
      "136 29.92474365234375\n",
      "137 28.150480270385742\n",
      "138 26.483835220336914\n",
      "139 24.91876792907715\n",
      "140 23.448396682739258\n",
      "141 22.068256378173828\n",
      "142 20.770294189453125\n",
      "143 19.551481246948242\n",
      "144 18.4067440032959\n",
      "145 17.329486846923828\n",
      "146 16.318143844604492\n",
      "147 15.367103576660156\n",
      "148 14.472803115844727\n",
      "149 13.631881713867188\n",
      "150 12.84206485748291\n",
      "151 12.098554611206055\n",
      "152 11.399113655090332\n",
      "153 10.740605354309082\n",
      "154 10.121891021728516\n",
      "155 9.53941535949707\n",
      "156 8.99145221710205\n",
      "157 8.475728988647461\n",
      "158 7.9908528327941895\n",
      "159 7.533819675445557\n",
      "160 7.103461742401123\n",
      "161 6.698513031005859\n",
      "162 6.317122936248779\n",
      "163 5.957971572875977\n",
      "164 5.619812965393066\n",
      "165 5.301689624786377\n",
      "166 5.001842975616455\n",
      "167 4.7192463874816895\n",
      "168 4.4527974128723145\n",
      "169 4.202044486999512\n",
      "170 3.965272903442383\n",
      "171 3.7426514625549316\n",
      "172 3.532756805419922\n",
      "173 3.3344197273254395\n",
      "174 3.1482934951782227\n",
      "175 2.972012758255005\n",
      "176 2.806259870529175\n",
      "177 2.6498935222625732\n",
      "178 2.5023787021636963\n",
      "179 2.363192319869995\n",
      "180 2.231903314590454\n",
      "181 2.1081700325012207\n",
      "182 1.9914586544036865\n",
      "183 1.8811938762664795\n",
      "184 1.7772858142852783\n",
      "185 1.6793768405914307\n",
      "186 1.5868266820907593\n",
      "187 1.4992250204086304\n",
      "188 1.4169398546218872\n",
      "189 1.3391103744506836\n",
      "190 1.2657660245895386\n",
      "191 1.1963812112808228\n",
      "192 1.1308649778366089\n",
      "193 1.06902277469635\n",
      "194 1.0106451511383057\n",
      "195 0.9555705189704895\n",
      "196 0.9036055207252502\n",
      "197 0.8542375564575195\n",
      "198 0.8080123066902161\n",
      "199 0.7640350461006165\n",
      "200 0.7227140069007874\n",
      "201 0.68340665102005\n",
      "202 0.6464979648590088\n",
      "203 0.6116405725479126\n",
      "204 0.5785466432571411\n",
      "205 0.5472338199615479\n",
      "206 0.5178142786026001\n",
      "207 0.4899064898490906\n",
      "208 0.4634981155395508\n",
      "209 0.43857741355895996\n",
      "210 0.4150524437427521\n",
      "211 0.3928226828575134\n",
      "212 0.37178951501846313\n",
      "213 0.35187143087387085\n",
      "214 0.33309727907180786\n",
      "215 0.31531935930252075\n",
      "216 0.2985098659992218\n",
      "217 0.28256362676620483\n",
      "218 0.26753684878349304\n",
      "219 0.253286212682724\n",
      "220 0.23984694480895996\n",
      "221 0.22707156836986542\n",
      "222 0.21507155895233154\n",
      "223 0.2036009579896927\n",
      "224 0.19279822707176208\n",
      "225 0.18260402977466583\n",
      "226 0.17294932901859283\n",
      "227 0.163839191198349\n",
      "228 0.15516433119773865\n",
      "229 0.14698182046413422\n",
      "230 0.13922494649887085\n",
      "231 0.13190008699893951\n",
      "232 0.12495562434196472\n",
      "233 0.11838451772928238\n",
      "234 0.11217885464429855\n",
      "235 0.10630074143409729\n",
      "236 0.10072832554578781\n",
      "237 0.09545010328292847\n",
      "238 0.0904458686709404\n",
      "239 0.0857190266251564\n",
      "240 0.08123670518398285\n",
      "241 0.07699772715568542\n",
      "242 0.07297717034816742\n",
      "243 0.06918273866176605\n",
      "244 0.06559403240680695\n",
      "245 0.06217221915721893\n",
      "246 0.058928295969963074\n",
      "247 0.05588424205780029\n",
      "248 0.052985917776823044\n",
      "249 0.05023679882287979\n",
      "250 0.04761014133691788\n",
      "251 0.04516301676630974\n",
      "252 0.04280893877148628\n",
      "253 0.04059405252337456\n",
      "254 0.03851375728845596\n",
      "255 0.03650093451142311\n",
      "256 0.03462660312652588\n",
      "257 0.032857414335012436\n",
      "258 0.03116084635257721\n",
      "259 0.029567888006567955\n",
      "260 0.028046412393450737\n",
      "261 0.02659921534359455\n",
      "262 0.025238579139113426\n",
      "263 0.023950422182679176\n",
      "264 0.02272341400384903\n",
      "265 0.021562714129686356\n",
      "266 0.020456161350011826\n",
      "267 0.01941358670592308\n",
      "268 0.01842096447944641\n",
      "269 0.01749320700764656\n",
      "270 0.016605239361524582\n",
      "271 0.01575988344848156\n",
      "272 0.014964723028242588\n",
      "273 0.01422282587736845\n",
      "274 0.013491598889231682\n",
      "275 0.012819159775972366\n",
      "276 0.012174585834145546\n",
      "277 0.011561490595340729\n",
      "278 0.010987245477735996\n",
      "279 0.010432273149490356\n",
      "280 0.009911814704537392\n",
      "281 0.009425722062587738\n",
      "282 0.008956242352724075\n",
      "283 0.008509268052875996\n",
      "284 0.008088486269116402\n",
      "285 0.007691225036978722\n",
      "286 0.0073121413588523865\n",
      "287 0.006957565434277058\n",
      "288 0.006617104634642601\n",
      "289 0.006289197131991386\n",
      "290 0.005987375974655151\n",
      "291 0.005696926731616259\n",
      "292 0.005420528817921877\n",
      "293 0.005157913081347942\n",
      "294 0.004911839030683041\n",
      "295 0.004681059624999762\n",
      "296 0.004452298395335674\n",
      "297 0.004242328926920891\n",
      "298 0.0040424540638923645\n",
      "299 0.0038494891487061977\n",
      "300 0.0036697075702250004\n",
      "301 0.00349216815084219\n",
      "302 0.0033285771496593952\n",
      "303 0.0031742434948682785\n",
      "304 0.0030278784688562155\n",
      "305 0.0028861723840236664\n",
      "306 0.00275416043587029\n",
      "307 0.0026293755508959293\n",
      "308 0.002513718092814088\n",
      "309 0.002398132113739848\n",
      "310 0.0022906765807420015\n",
      "311 0.0021877302788197994\n",
      "312 0.0020924911368638277\n",
      "313 0.001999997068196535\n",
      "314 0.0019120237557217479\n",
      "315 0.0018285043770447373\n",
      "316 0.0017471866449341178\n",
      "317 0.001672935439273715\n",
      "318 0.0016014821594581008\n",
      "319 0.0015324712730944157\n",
      "320 0.0014696724247187376\n",
      "321 0.001405993476510048\n",
      "322 0.0013480267953127623\n",
      "323 0.0012918604770675302\n",
      "324 0.0012401221320033073\n",
      "325 0.0011897702934220433\n",
      "326 0.0011419549118727446\n",
      "327 0.0010948546696454287\n",
      "328 0.0010510911233723164\n",
      "329 0.0010095888283103704\n",
      "330 0.0009729510056786239\n",
      "331 0.0009327293373644352\n",
      "332 0.0008982945000752807\n",
      "333 0.0008630093652755022\n",
      "334 0.0008309785625897348\n",
      "335 0.0007994232000783086\n",
      "336 0.0007695745443925261\n",
      "337 0.0007401289767585695\n",
      "338 0.0007124026888050139\n",
      "339 0.0006869662320241332\n",
      "340 0.000662296311929822\n",
      "341 0.0006385609740391374\n",
      "342 0.0006161407800391316\n",
      "343 0.0005939261754974723\n",
      "344 0.0005735853919759393\n",
      "345 0.0005528434412553906\n",
      "346 0.0005346171674318612\n",
      "347 0.0005162462475709617\n",
      "348 0.000499196641612798\n",
      "349 0.0004811344260815531\n",
      "350 0.0004663243889808655\n",
      "351 0.0004503024974837899\n",
      "352 0.0004357858852017671\n",
      "353 0.000422078650444746\n",
      "354 0.0004084838437847793\n",
      "355 0.00039446671144105494\n",
      "356 0.0003824504092335701\n",
      "357 0.00036909652408212423\n",
      "358 0.00035836538881994784\n",
      "359 0.00034782884176820517\n",
      "360 0.00033630477264523506\n",
      "361 0.0003262143873143941\n",
      "362 0.00031641111127100885\n",
      "363 0.00030749241705052555\n",
      "364 0.0002974704257212579\n",
      "365 0.00028871733229607344\n",
      "366 0.0002799350186251104\n",
      "367 0.0002717047755140811\n",
      "368 0.0002634631819091737\n",
      "369 0.0002567930205259472\n",
      "370 0.0002492535568308085\n",
      "371 0.0002419194352114573\n",
      "372 0.0002357177872909233\n",
      "373 0.0002290735865244642\n",
      "374 0.00022260824334807694\n",
      "375 0.00021622490021400154\n",
      "376 0.00021061298321001232\n",
      "377 0.0002052966592600569\n",
      "378 0.00020020006923004985\n",
      "379 0.00019501461065374315\n",
      "380 0.00018986295617651194\n",
      "381 0.00018486383487470448\n",
      "382 0.00018036493565887213\n",
      "383 0.00017511448822915554\n",
      "384 0.00017033987387549132\n",
      "385 0.00016670090553816408\n",
      "386 0.000162016338435933\n",
      "387 0.00015821224951650947\n",
      "388 0.0001546704734209925\n",
      "389 0.00015104992780834436\n",
      "390 0.00014721593470312655\n",
      "391 0.00014325356460176408\n",
      "392 0.0001398776366841048\n",
      "393 0.00013631675392389297\n",
      "394 0.0001336924615316093\n",
      "395 0.00013044809747952968\n",
      "396 0.0001273498492082581\n",
      "397 0.0001240805722773075\n",
      "398 0.00012145435903221369\n",
      "399 0.00011911577166756615\n",
      "400 0.0001168059025076218\n",
      "401 0.0001137389917857945\n",
      "402 0.00011083210119977593\n",
      "403 0.00010869401739910245\n",
      "404 0.00010640551045071334\n",
      "405 0.00010425737855257466\n",
      "406 0.00010199897224083543\n",
      "407 0.00010020504123531282\n",
      "408 9.795200458029285e-05\n",
      "409 9.619278716854751e-05\n",
      "410 9.384707664139569e-05\n",
      "411 9.24050182220526e-05\n",
      "412 9.086703357752413e-05\n",
      "413 8.847150456858799e-05\n",
      "414 8.666700887260959e-05\n",
      "415 8.516159141436219e-05\n",
      "416 8.375506149604917e-05\n",
      "417 8.200841693906114e-05\n",
      "418 8.032951154746115e-05\n",
      "419 7.911171269370243e-05\n",
      "420 7.749701035209e-05\n",
      "421 7.611783075844869e-05\n",
      "422 7.4722760473378e-05\n",
      "423 7.334363181143999e-05\n",
      "424 7.210606418084353e-05\n",
      "425 7.085579272825271e-05\n",
      "426 6.964981730561703e-05\n",
      "427 6.816811219323426e-05\n",
      "428 6.701307574985549e-05\n",
      "429 6.58048375044018e-05\n",
      "430 6.446350744226947e-05\n",
      "431 6.358021346386522e-05\n",
      "432 6.231097358977422e-05\n",
      "433 6.114480493124574e-05\n",
      "434 6.0300859331618994e-05\n",
      "435 5.914361827308312e-05\n",
      "436 5.8098510635318235e-05\n",
      "437 5.715896622859873e-05\n",
      "438 5.6031360145425424e-05\n",
      "439 5.51621888007503e-05\n",
      "440 5.399261135607958e-05\n",
      "441 5.307427636580542e-05\n",
      "442 5.239477104623802e-05\n",
      "443 5.168151983525604e-05\n",
      "444 5.0714679673546925e-05\n",
      "445 5.004719059797935e-05\n",
      "446 4.9293405027128756e-05\n",
      "447 4.838192762690596e-05\n",
      "448 4.7709978389320895e-05\n",
      "449 4.683216320700012e-05\n",
      "450 4.608398012351245e-05\n",
      "451 4.549749428406358e-05\n",
      "452 4.492725929594599e-05\n",
      "453 4.4066953705623746e-05\n",
      "454 4.338270809967071e-05\n",
      "455 4.2654326534830034e-05\n",
      "456 4.2072129872394726e-05\n",
      "457 4.131835885345936e-05\n",
      "458 4.094160976819694e-05\n",
      "459 4.040946078021079e-05\n",
      "460 3.982945781899616e-05\n",
      "461 3.930013917852193e-05\n",
      "462 3.862919766106643e-05\n",
      "463 3.795621159952134e-05\n",
      "464 3.755572470254265e-05\n",
      "465 3.710310920723714e-05\n",
      "466 3.64943225577008e-05\n",
      "467 3.6176934372633696e-05\n",
      "468 3.579714393708855e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 3.5320430470164865e-05\n",
      "470 3.476472193142399e-05\n",
      "471 3.4370783396298066e-05\n",
      "472 3.378089240868576e-05\n",
      "473 3.342274794704281e-05\n",
      "474 3.2946394640021026e-05\n",
      "475 3.23609565384686e-05\n",
      "476 3.1946576200425625e-05\n",
      "477 3.1353974918602034e-05\n",
      "478 3.1048715754877776e-05\n",
      "479 3.069139347644523e-05\n",
      "480 3.019629730260931e-05\n",
      "481 2.9947921575512737e-05\n",
      "482 2.951656460936647e-05\n",
      "483 2.9135610020603053e-05\n",
      "484 2.910097646235954e-05\n",
      "485 2.8637467039516196e-05\n",
      "486 2.825950286933221e-05\n",
      "487 2.789109930745326e-05\n",
      "488 2.751221472863108e-05\n",
      "489 2.7171981855644844e-05\n",
      "490 2.6808334951056167e-05\n",
      "491 2.649083580763545e-05\n",
      "492 2.6120926122530364e-05\n",
      "493 2.586271148175001e-05\n",
      "494 2.552783553255722e-05\n",
      "495 2.5130539142992347e-05\n",
      "496 2.5036015358637087e-05\n",
      "497 2.4712815502425656e-05\n",
      "498 2.4380844479310326e-05\n",
      "499 2.416023926343769e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Randomly initialize weights\n",
    "w1 = torch.randn(D_in, H, device=device)\n",
    "w2 = torch.randn(H, D_out, device=device)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y\n",
    "  h = x.mm(w1)\n",
    "  h_relu = h.clamp(min=0)\n",
    "  y_pred = h_relu.mm(w2)\n",
    "\n",
    "  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor\n",
    "  # of shape (); we can get its value as a Python number with loss.item().\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "  grad_y_pred = 2.0 * (y_pred - y)\n",
    "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "  grad_h = grad_h_relu.clone()\n",
    "  grad_h[h < 0] = 0\n",
    "  grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "  # Update weights using gradient descent\n",
    "  w1 -= learning_rate * grad_w1\n",
    "  w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph and Autograd\n",
    "\n",
    "to be discussed in detail ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of w 1  w.r.t to L: tensor(-36.)\n",
      "Gradient of w 2  w.r.t to L: tensor(-28.)\n",
      "Gradient of w 3  w.r.t to L: tensor(-8.)\n",
      "Gradient of w 4  w.r.t to L: tensor(-20.)\n"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Define the leaf nodes\n",
    "a = Variable(FloatTensor([4]))\n",
    "\n",
    "weights = [Variable(FloatTensor([i]), requires_grad=True) for i in (2, 5, 9, 7)]\n",
    "\n",
    "# unpack the weights for nicer assignment\n",
    "w1, w2, w3, w4 = weights\n",
    "\n",
    "b = w1 * a\n",
    "c = w2 * a\n",
    "d = w3 * b + w4 * c\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()\n",
    "\n",
    "for index, weight in enumerate(weights, start=1):\n",
    "    gradient, *_ = weight.grad.data\n",
    "    print(\"Gradient of w\",index,\" w.r.t to L:\", gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch : NN using Autograd\n",
    "\n",
    "In the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks.\n",
    "\n",
    "Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients.\n",
    "\n",
    "This sounds complicated, it's pretty simple to use in practice. If we want to compute gradients with respect to some Tensor, then we set requires_grad=True when constructing that Tensor. Any PyTorch operations on that Tensor will cause a computational graph to be constructed, allowing us to later perform backpropagation through the graph. If x is a Tensor with requires_grad=True, then after backpropagation x.grad will be another Tensor holding the gradient of x with respect to some scalar value.\n",
    "\n",
    "Sometimes you may wish to prevent PyTorch from building computational graphs when performing certain operations on Tensors with requires_grad=True; for example we usually don't want to backpropagate through the weight update steps when training a neural network. In such scenarios we can use the torch.no_grad() context manager to prevent the construction of a computational graph.\n",
    "\n",
    "Here we use PyTorch Tensors and autograd to implement our two-layer network; now we no longer need to manually implement the backward pass through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28461560.0\n",
      "1 24581592.0\n",
      "2 25326950.0\n",
      "3 26960664.0\n",
      "4 26731064.0\n",
      "5 22816684.0\n",
      "6 16503462.0\n",
      "7 10222507.0\n",
      "8 5831145.0\n",
      "9 3273193.5\n",
      "10 1939987.0\n",
      "11 1255465.625\n",
      "12 892390.25\n",
      "13 684106.375\n",
      "14 552766.1875\n",
      "15 461715.75\n",
      "16 393643.90625\n",
      "17 339990.78125\n",
      "18 296197.75\n",
      "19 259693.625\n",
      "20 228803.078125\n",
      "21 202427.609375\n",
      "22 179759.453125\n",
      "23 160171.65625\n",
      "24 143134.515625\n",
      "25 128253.15625\n",
      "26 115216.0234375\n",
      "27 103752.015625\n",
      "28 93644.7734375\n",
      "29 84717.6640625\n",
      "30 76796.7734375\n",
      "31 69750.953125\n",
      "32 63467.5\n",
      "33 57856.00390625\n",
      "34 52831.71484375\n",
      "35 48324.32421875\n",
      "36 44273.53125\n",
      "37 40624.16796875\n",
      "38 37329.21875\n",
      "39 34349.96875\n",
      "40 31652.16796875\n",
      "41 29204.92578125\n",
      "42 26981.228515625\n",
      "43 24958.677734375\n",
      "44 23116.654296875\n",
      "45 21436.1015625\n",
      "46 19899.953125\n",
      "47 18493.55859375\n",
      "48 17204.76171875\n",
      "49 16022.283203125\n",
      "50 14937.9365234375\n",
      "51 13941.734375\n",
      "52 13024.3720703125\n",
      "53 12178.15234375\n",
      "54 11396.8916015625\n",
      "55 10674.86328125\n",
      "56 10006.75390625\n",
      "57 9387.90625\n",
      "58 8814.1484375\n",
      "59 8281.537109375\n",
      "60 7786.8193359375\n",
      "61 7327.1962890625\n",
      "62 6899.3955078125\n",
      "63 6500.9970703125\n",
      "64 6129.6904296875\n",
      "65 5783.30224609375\n",
      "66 5459.84423828125\n",
      "67 5157.6044921875\n",
      "68 4874.94189453125\n",
      "69 4610.50537109375\n",
      "70 4362.7958984375\n",
      "71 4130.54736328125\n",
      "72 3912.636474609375\n",
      "73 3708.11767578125\n",
      "74 3516.05908203125\n",
      "75 3335.67919921875\n",
      "76 3166.003662109375\n",
      "77 3006.23046875\n",
      "78 2855.780517578125\n",
      "79 2714.11279296875\n",
      "80 2580.4853515625\n",
      "81 2454.421142578125\n",
      "82 2335.371337890625\n",
      "83 2222.9130859375\n",
      "84 2116.6650390625\n",
      "85 2016.155029296875\n",
      "86 1921.0814208984375\n",
      "87 1831.085205078125\n",
      "88 1745.8533935546875\n",
      "89 1665.100830078125\n",
      "90 1588.6119384765625\n",
      "91 1516.0513916015625\n",
      "92 1447.185546875\n",
      "93 1381.8306884765625\n",
      "94 1319.784423828125\n",
      "95 1260.864990234375\n",
      "96 1204.884765625\n",
      "97 1151.6573486328125\n",
      "98 1101.037841796875\n",
      "99 1052.8896484375\n",
      "100 1007.068603515625\n",
      "101 963.5408935546875\n",
      "102 922.0968017578125\n",
      "103 882.6105346679688\n",
      "104 844.9782104492188\n",
      "105 809.103515625\n",
      "106 774.905517578125\n",
      "107 742.2774658203125\n",
      "108 711.1478271484375\n",
      "109 681.4325561523438\n",
      "110 653.073486328125\n",
      "111 626.0009765625\n",
      "112 600.1398315429688\n",
      "113 575.423583984375\n",
      "114 551.8035888671875\n",
      "115 529.2342529296875\n",
      "116 507.65557861328125\n",
      "117 487.0169982910156\n",
      "118 467.2772521972656\n",
      "119 448.3939208984375\n",
      "120 430.33111572265625\n",
      "121 413.0411376953125\n",
      "122 396.49346923828125\n",
      "123 380.6534423828125\n",
      "124 365.4833679199219\n",
      "125 350.962158203125\n",
      "126 337.0521240234375\n",
      "127 323.7220458984375\n",
      "128 310.95050048828125\n",
      "129 298.7109375\n",
      "130 286.9833068847656\n",
      "131 275.73541259765625\n",
      "132 264.9505615234375\n",
      "133 254.61683654785156\n",
      "134 244.70237731933594\n",
      "135 235.1892852783203\n",
      "136 226.06552124023438\n",
      "137 217.311279296875\n",
      "138 208.9119873046875\n",
      "139 200.8500213623047\n",
      "140 193.1116485595703\n",
      "141 185.68478393554688\n",
      "142 178.55763244628906\n",
      "143 171.71343994140625\n",
      "144 165.14248657226562\n",
      "145 158.83248901367188\n",
      "146 152.77313232421875\n",
      "147 146.95272827148438\n",
      "148 141.36087036132812\n",
      "149 135.98809814453125\n",
      "150 130.82937622070312\n",
      "151 125.87129211425781\n",
      "152 121.10842895507812\n",
      "153 116.5309829711914\n",
      "154 112.1331558227539\n",
      "155 107.90843200683594\n",
      "156 103.843994140625\n",
      "157 99.93675994873047\n",
      "158 96.1815414428711\n",
      "159 92.57278442382812\n",
      "160 89.10083770751953\n",
      "161 85.76364135742188\n",
      "162 82.5550765991211\n",
      "163 79.46856689453125\n",
      "164 76.50079345703125\n",
      "165 73.6485595703125\n",
      "166 70.90513610839844\n",
      "167 68.26419830322266\n",
      "168 65.72463989257812\n",
      "169 63.28112030029297\n",
      "170 60.930870056152344\n",
      "171 58.670692443847656\n",
      "172 56.49517059326172\n",
      "173 54.40245819091797\n",
      "174 52.38882064819336\n",
      "175 50.45180892944336\n",
      "176 48.587303161621094\n",
      "177 46.793800354003906\n",
      "178 45.06719970703125\n",
      "179 43.40580368041992\n",
      "180 41.80626678466797\n",
      "181 40.266265869140625\n",
      "182 38.78570556640625\n",
      "183 37.35897445678711\n",
      "184 35.98618698120117\n",
      "185 34.66480255126953\n",
      "186 33.39187240600586\n",
      "187 32.1685905456543\n",
      "188 30.989051818847656\n",
      "189 29.853824615478516\n",
      "190 28.761520385742188\n",
      "191 27.70877456665039\n",
      "192 26.69536590576172\n",
      "193 25.719738006591797\n",
      "194 24.780567169189453\n",
      "195 23.87582015991211\n",
      "196 23.004425048828125\n",
      "197 22.16588020324707\n",
      "198 21.358661651611328\n",
      "199 20.580753326416016\n",
      "200 19.831207275390625\n",
      "201 19.109386444091797\n",
      "202 18.413686752319336\n",
      "203 17.744585037231445\n",
      "204 17.099689483642578\n",
      "205 16.478696823120117\n",
      "206 15.880691528320312\n",
      "207 15.304300308227539\n",
      "208 14.748673439025879\n",
      "209 14.214300155639648\n",
      "210 13.69951057434082\n",
      "211 13.20294189453125\n",
      "212 12.724996566772461\n",
      "213 12.264199256896973\n",
      "214 11.820446968078613\n",
      "215 11.393058776855469\n",
      "216 10.98130989074707\n",
      "217 10.584474563598633\n",
      "218 10.202107429504395\n",
      "219 9.833436965942383\n",
      "220 9.478830337524414\n",
      "221 9.136943817138672\n",
      "222 8.80745792388916\n",
      "223 8.490015029907227\n",
      "224 8.184151649475098\n",
      "225 7.889086723327637\n",
      "226 7.60488224029541\n",
      "227 7.331094741821289\n",
      "228 7.067401885986328\n",
      "229 6.812849521636963\n",
      "230 6.5680694580078125\n",
      "231 6.332057952880859\n",
      "232 6.104650497436523\n",
      "233 5.885191917419434\n",
      "234 5.67366886138916\n",
      "235 5.470315933227539\n",
      "236 5.273515224456787\n",
      "237 5.084353923797607\n",
      "238 4.901991844177246\n",
      "239 4.726441860198975\n",
      "240 4.556641101837158\n",
      "241 4.393526554107666\n",
      "242 4.236224174499512\n",
      "243 4.084514617919922\n",
      "244 3.9383339881896973\n",
      "245 3.797328472137451\n",
      "246 3.661491632461548\n",
      "247 3.5305063724517822\n",
      "248 3.404210090637207\n",
      "249 3.2824020385742188\n",
      "250 3.165010929107666\n",
      "251 3.0519914627075195\n",
      "252 2.9429285526275635\n",
      "253 2.8379077911376953\n",
      "254 2.736516237258911\n",
      "255 2.6389546394348145\n",
      "256 2.5447323322296143\n",
      "257 2.4538841247558594\n",
      "258 2.366413116455078\n",
      "259 2.281956434249878\n",
      "260 2.2005913257598877\n",
      "261 2.1220409870147705\n",
      "262 2.0466365814208984\n",
      "263 1.9737673997879028\n",
      "264 1.9033218622207642\n",
      "265 1.8357980251312256\n",
      "266 1.7703931331634521\n",
      "267 1.7073771953582764\n",
      "268 1.6466829776763916\n",
      "269 1.5881165266036987\n",
      "270 1.5316765308380127\n",
      "271 1.4772071838378906\n",
      "272 1.4246002435684204\n",
      "273 1.3739323616027832\n",
      "274 1.3252265453338623\n",
      "275 1.2780094146728516\n",
      "276 1.232745885848999\n",
      "277 1.1889357566833496\n",
      "278 1.1467450857162476\n",
      "279 1.1060163974761963\n",
      "280 1.066789150238037\n",
      "281 1.0289335250854492\n",
      "282 0.9924885034561157\n",
      "283 0.9573283195495605\n",
      "284 0.9234151840209961\n",
      "285 0.8906071186065674\n",
      "286 0.8590409755706787\n",
      "287 0.8286443948745728\n",
      "288 0.7992880940437317\n",
      "289 0.7710551619529724\n",
      "290 0.7437672019004822\n",
      "291 0.7173987030982971\n",
      "292 0.6920662522315979\n",
      "293 0.6675810813903809\n",
      "294 0.6439613103866577\n",
      "295 0.6211202144622803\n",
      "296 0.5992072224617004\n",
      "297 0.5780544281005859\n",
      "298 0.5575990676879883\n",
      "299 0.5378565788269043\n",
      "300 0.5188873410224915\n",
      "301 0.500583827495575\n",
      "302 0.4828668534755707\n",
      "303 0.46580326557159424\n",
      "304 0.449413925409317\n",
      "305 0.4335286021232605\n",
      "306 0.4182471036911011\n",
      "307 0.4034399092197418\n",
      "308 0.3892611563205719\n",
      "309 0.3755302429199219\n",
      "310 0.3623243570327759\n",
      "311 0.34950122237205505\n",
      "312 0.33724042773246765\n",
      "313 0.3253226578235626\n",
      "314 0.31385546922683716\n",
      "315 0.30279847979545593\n",
      "316 0.2921754717826843\n",
      "317 0.281888484954834\n",
      "318 0.27197015285491943\n",
      "319 0.26238420605659485\n",
      "320 0.2531821131706238\n",
      "321 0.24427323043346405\n",
      "322 0.23564040660858154\n",
      "323 0.22739893198013306\n",
      "324 0.21937619149684906\n",
      "325 0.21171367168426514\n",
      "326 0.20425114035606384\n",
      "327 0.19703537225723267\n",
      "328 0.1901170313358307\n",
      "329 0.18344122171401978\n",
      "330 0.17699626088142395\n",
      "331 0.17079147696495056\n",
      "332 0.16482582688331604\n",
      "333 0.15902014076709747\n",
      "334 0.15346366167068481\n",
      "335 0.1480913907289505\n",
      "336 0.14288708567619324\n",
      "337 0.13789479434490204\n",
      "338 0.13305410742759705\n",
      "339 0.12841878831386566\n",
      "340 0.12388080358505249\n",
      "341 0.11954820156097412\n",
      "342 0.11537688225507736\n",
      "343 0.11132638901472092\n",
      "344 0.10744152963161469\n",
      "345 0.10370196402072906\n",
      "346 0.10008352994918823\n",
      "347 0.09657037258148193\n",
      "348 0.09318285435438156\n",
      "349 0.08992677927017212\n",
      "350 0.08678452670574188\n",
      "351 0.08375968784093857\n",
      "352 0.08082956075668335\n",
      "353 0.07801971584558487\n",
      "354 0.07528050243854523\n",
      "355 0.0726839005947113\n",
      "356 0.07012329995632172\n",
      "357 0.06768900156021118\n",
      "358 0.06532551348209381\n",
      "359 0.06302864849567413\n",
      "360 0.06081312149763107\n",
      "361 0.058712877333164215\n",
      "362 0.05666801705956459\n",
      "363 0.054701514542102814\n",
      "364 0.05279041454195976\n",
      "365 0.05095142871141434\n",
      "366 0.04918600618839264\n",
      "367 0.04746989905834198\n",
      "368 0.04581531882286072\n",
      "369 0.04422927647829056\n",
      "370 0.04270549118518829\n",
      "371 0.04121360555291176\n",
      "372 0.03977976739406586\n",
      "373 0.0384063646197319\n",
      "374 0.03706621006131172\n",
      "375 0.035787031054496765\n",
      "376 0.034552622586488724\n",
      "377 0.033350542187690735\n",
      "378 0.03219347074627876\n",
      "379 0.03107781894505024\n",
      "380 0.03000476397573948\n",
      "381 0.028976934030652046\n",
      "382 0.027990257367491722\n",
      "383 0.027021054178476334\n",
      "384 0.026085160672664642\n",
      "385 0.025185229256749153\n",
      "386 0.024326153099536896\n",
      "387 0.023495079949498177\n",
      "388 0.022686343640089035\n",
      "389 0.02189689502120018\n",
      "390 0.021148018538951874\n",
      "391 0.020423835143446922\n",
      "392 0.01972588151693344\n",
      "393 0.019055791199207306\n",
      "394 0.01840297132730484\n",
      "395 0.01777728460729122\n",
      "396 0.01717453822493553\n",
      "397 0.016590451821684837\n",
      "398 0.016017261892557144\n",
      "399 0.015469410456717014\n",
      "400 0.01495148055255413\n",
      "401 0.014446635730564594\n",
      "402 0.013951078988611698\n",
      "403 0.013474156148731709\n",
      "404 0.013029447756707668\n",
      "405 0.012584428302943707\n",
      "406 0.012159238569438457\n",
      "407 0.01174438651651144\n",
      "408 0.011350654065608978\n",
      "409 0.010976126417517662\n",
      "410 0.010602409020066261\n",
      "411 0.010248418897390366\n",
      "412 0.009900657460093498\n",
      "413 0.009569329209625721\n",
      "414 0.009245616383850574\n",
      "415 0.00894090160727501\n",
      "416 0.008644085377454758\n",
      "417 0.008360560983419418\n",
      "418 0.008079741150140762\n",
      "419 0.007812368217855692\n",
      "420 0.007552857976406813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 0.007303833961486816\n",
      "422 0.007061677519232035\n",
      "423 0.006836325861513615\n",
      "424 0.006610485725104809\n",
      "425 0.006393070798367262\n",
      "426 0.006182613782584667\n",
      "427 0.005979666952043772\n",
      "428 0.005785953253507614\n",
      "429 0.005599744617938995\n",
      "430 0.0054175653494894505\n",
      "431 0.005245325621217489\n",
      "432 0.005077387671917677\n",
      "433 0.004912861622869968\n",
      "434 0.004753958433866501\n",
      "435 0.004603030160069466\n",
      "436 0.004459579475224018\n",
      "437 0.004314121324568987\n",
      "438 0.00417733658105135\n",
      "439 0.00404431251809001\n",
      "440 0.00392084289342165\n",
      "441 0.003794789081439376\n",
      "442 0.0036796829663217068\n",
      "443 0.003560226410627365\n",
      "444 0.003451234195381403\n",
      "445 0.0033441849518567324\n",
      "446 0.0032423967495560646\n",
      "447 0.003138990607112646\n",
      "448 0.003044123761355877\n",
      "449 0.0029521333053708076\n",
      "450 0.002860385226085782\n",
      "451 0.002772762905806303\n",
      "452 0.0026870639994740486\n",
      "453 0.0026062503457069397\n",
      "454 0.0025317114777863026\n",
      "455 0.002457126509398222\n",
      "456 0.002383872400969267\n",
      "457 0.0023117694072425365\n",
      "458 0.00224283616989851\n",
      "459 0.0021787809673696756\n",
      "460 0.0021138032898306847\n",
      "461 0.0020509613677859306\n",
      "462 0.001991591416299343\n",
      "463 0.0019324090098962188\n",
      "464 0.0018756045028567314\n",
      "465 0.0018223270308226347\n",
      "466 0.0017734052380546927\n",
      "467 0.00172255071811378\n",
      "468 0.0016737510450184345\n",
      "469 0.0016282927244901657\n",
      "470 0.00158158247359097\n",
      "471 0.0015376067021861672\n",
      "472 0.0014949198812246323\n",
      "473 0.0014522243291139603\n",
      "474 0.0014130559284240007\n",
      "475 0.0013751127989962697\n",
      "476 0.0013387316139414907\n",
      "477 0.001302516320720315\n",
      "478 0.0012661146465688944\n",
      "479 0.0012322135735303164\n",
      "480 0.0011997444089502096\n",
      "481 0.0011680159950628877\n",
      "482 0.0011361307697370648\n",
      "483 0.0011065062135457993\n",
      "484 0.0010787125211209059\n",
      "485 0.0010495797032490373\n",
      "486 0.0010218399111181498\n",
      "487 0.0009942069882526994\n",
      "488 0.0009686807752586901\n",
      "489 0.0009452938684262335\n",
      "490 0.0009207547409459949\n",
      "491 0.0008963124710135162\n",
      "492 0.000875196186825633\n",
      "493 0.000854067038744688\n",
      "494 0.0008328062249347568\n",
      "495 0.0008121486753225327\n",
      "496 0.0007913411827757955\n",
      "497 0.0007702860166318715\n",
      "498 0.000753477041143924\n",
      "499 0.0007356920395977795\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda') # Uncomment this to run on GPU\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold input and outputs\n",
    "x = torch.randn(N, D_in, device=device)\n",
    "y = torch.randn(N, D_out, device=device)\n",
    "\n",
    "# Create random Tensors for weights; setting requires_grad=True means that we\n",
    "# want to compute gradients for these Tensors during the backward pass.\n",
    "w1 = torch.randn(D_in, H, device=device, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "  # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "  # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "  # PyTorch to build a computational graph, allowing automatic computation of\n",
    "  # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "  # don't need to keep references to intermediate values.\n",
    "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "  \n",
    "  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()\n",
    "  # is a Python number giving its value.\n",
    "  loss = (y_pred - y).pow(2).sum()\n",
    "  print(t, loss.item())\n",
    "\n",
    "  # Use autograd to compute the backward pass. This call will compute the\n",
    "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
    "  # of the loss with respect to w1 and w2 respectively.\n",
    "  loss.backward()\n",
    "\n",
    "  # Update weights using gradient descent. For this step we just want to mutate\n",
    "  # the values of w1 and w2 in-place; we don't want to build up a computational\n",
    "  # graph for the update steps, so we use the torch.no_grad() context manager\n",
    "  # to prevent PyTorch from building a computational graph for the updates\n",
    "  with torch.no_grad():\n",
    "    w1 -= learning_rate * w1.grad\n",
    "    w2 -= learning_rate * w2.grad\n",
    "\n",
    "    # Manually zero the gradients after running the backward pass\n",
    "    w1.grad.zero_()\n",
    "    w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't worry if you didn't understand anything above, everything will be explanied much clearly as we go through the next set of notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
